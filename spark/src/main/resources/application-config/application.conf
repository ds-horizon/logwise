// application config
app.job.name = push-logs-to-s3

// kafka stream config
kafka.cluster.dns = kafka
kafka.bootstrap.servers.port = 9092
kafka.maxRatePerPartition = 4000
kafka.startingOffsets = latest
kafka.startingOffsetsTimestamp = 0
kafka.topic.prefix.application = "^logs.*"

// s3 config
s3.dir.checkpoint = checkpoint
s3.dir.logs = logs
s3.uri.scheme = "s3a://"
s3.path.checkpoint.application = ${s3.uri.scheme}${s3.bucket}"/"${s3.dir.checkpoint}"/application"
s3.path.logs.application = ${s3.uri.scheme}${s3.bucket}"/"${s3.dir.logs}

// spark application config
spark.offsetPerTrigger.default = 750000000
spark.processing.time.seconds = 60
spark.streamingquery.timeout.minutes = 25
spark.streams.name = ["application-logs-stream-to-s3"]

// spark context config
sparkConfig."checkpointIntervalInSeconds" = 60
sparkConfig."fs.s3.maxRetries" = 2
sparkConfig."spark.app.name" = log-management
sparkConfig."spark.dynamicAllocation.enabled" = true
sparkConfig."spark.dynamicAllocation.executorAllocationRatio" = 0.75
sparkConfig."spark.dynamicAllocation.shuffleTracking.enabled" = true
sparkConfig."spark.kryo.unsafe" = true
sparkConfig."spark.memory.fraction" = 0.8
sparkConfig."spark.memory.storageFraction" = 0.3
sparkConfig."spark.network.timeout" = 240
sparkConfig."spark.rpc.numRetries" = 5
sparkConfig."spark.serializer" = org.apache.spark.serializer.KryoSerializer
sparkConfig."spark.shuffle.service.enabled" = true
sparkConfig."spark.sql.adaptive.advisoryPartitionSizeInBytes" = 128000000
sparkConfig."spark.sql.adaptive.coalescePartitions.enabled" = true
sparkConfig."spark.sql.adaptive.enabled" = true
sparkConfig."spark.sql.hive.metastorePartitionPruning" = true
sparkConfig."spark.sql.inMemoryColumnarStorage.batchSize" = 1000000
sparkConfig."spark.sql.inMemoryColumnarStorage.compressed" = true
sparkConfig."spark.sql.parquet.columnarReaderBatchSize" = 4096
sparkConfig."spark.sql.parquet.compression.codes" = gzip
sparkConfig."spark.sql.parquet.enableVectorizedReader" = true
sparkConfig."spark.sql.parquet.filterPushdown" = true
sparkConfig."spark.sql.parquet.mergeSchema" = false
sparkConfig."spark.sql.shuffle.partitions" = 2
sparkConfig."spark.sql.sources.compression" = gzip
sparkConfig."spark.sql.streaming.fileSource.log.compactInterval" = 15
sparkConfig."spark.sql.streaming.metricsEnabled" = true
sparkConfig."spark.sql.streaming.minBatchesToRetain" = 20
sparkConfig."spark.streaming.backpressure.enabled" = true
sparkConfig."spark.streaming.concurrentJobs" = 4
sparkConfig."spark.kafka.consumer.cache.capacity" = 256

// spark hadoop config
sparkConfig."spark.hadoop.fs.s3a.committer.magic.enabled" = true
sparkConfig."spark.hadoop.fs.s3a.committer.name" = magic
sparkConfig."spark.hadoop.fs.s3a.path.style.access" = true
sparkConfig."spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version" = 2
sparkConfig."spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored" = true
sparkConfig."spark.hadoop.parquet.enable.summary-metadata" = false
